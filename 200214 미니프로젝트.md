[toc]

# Day40 미니프로젝트(5)

## 1 . 수행평가 1번 문제와 동일, 제출
IMDB 데이터셋 사용(주석을 필히 작성)

- 리뷰 데이터 전처리(불용어, 특수문자, 숫자 등 제거, 어근 동일화 등)
- DTM 생성(문서-단어 행렬)
- train/test data set 분리
- 열의 크기가 매우 크므로 최소 10번(변경) 이상 등장한 단어 대상으로 할 것
(5만개의 데이터 중 일부를 가지고 작업 수행 가능)
- 로컬, 코렙

```R
# data 문자열로 부르기
# 이상한 한글로 읽혀지는 문자들이 존재하여, txt 파일로 저장 후 호출
imdb = read.csv("imdb-dataset-of-50k-movie-reviews/IMDB Dataset.txt", 
                encoding = "UTF-8", stringsAsFactors = FALSE)
```

```R
# 12000개의 데이터 사용
imdb_sub = imdb[1:12000, ]
str(imdb_sub)
## 'data.frame':    12000 obs. of  2 variables:
##  $ review   : chr  "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right"| __truncated__ "A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion "| __truncated__ "I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned th"| __truncated__ "Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fi"| __truncated__ ...
##  $ sentiment: chr  "positive" "positive" "positive" "negative" ...
```

```R
# sentiment만 factor로 변경
imdb_sub$sentiment = as.factor(imdb_sub$sentiment)
str(imdb_sub$sentiment)
##  Factor w/ 2 levels "negative","positive": 2 2 2 1 2 2 2 1 1 2 ...
```

```R
dim(imdb_sub)
## [1] 12000     2
```

```R
library(stringr)
# <br /> 먼저 제거
imdb_sub$review = str_remove_all(imdb_sub$review, "<br />")
```

```R
library(tm)

# 분석을 위한 corpus 생성
imdb_corpus = VCorpus(VectorSource(imdb_sub$review))
```

```R
# stemming 
imdb_corpus_clean = tm_map(imdb_corpus, stemDocument)

# 토큰화
imdb_dtm = DocumentTermMatrix(
  imdb_corpus, control = list(tolower = TRUE,
                              removeNumbers = TRUE,
                              stopwords = TRUE,
                              removePunctuation = TRUE,
                              stemming = TRUE))
imdb_dtm
## <<DocumentTermMatrix (documents: 12000, terms: 68626)>>
## Non-/sparse entries: 1136197/822375803
## Sparsity           : 100%
## Maximal term length: 64
## Weighting          : term frequency (tf)
```

```R
inspect(imdb_dtm)
## <<DocumentTermMatrix (documents: 12000, terms: 68626)>>
## Non-/sparse entries: 1136197/822375803
## Sparsity           : 100%
## Maximal term length: 64
## Weighting          : term frequency (tf)
## Sample             :
##        Terms
## Docs    film get good just like make movi one see time
##   10115    0   3    2    1    1    1    1   5   0    1
##   10364    0  17    1    1    0    2    0   2   0    2
##   1532    25   1    5   10    7    3    2  16   4    6
##   3025     8   6    2    5    7    1    2   8   3    2
##   3655     1   3    2    2    4    1   14   3   4    4
##   557      7   3    5    6   10    2   17   4   6    1
##   5660     5   1    0    1    2    2    3   1   2    2
##   5709    24   5    3    3    8    3    0  10   3    5
##   7900    30   3    4    4    4    3    0   6   1    3
##   8221     3   9    1    5    6    2   22   9   1    3
```

```R
# 최소 100번 이상 등장한 단어 찾기
imdb_freq_words = findFreqTerms(imdb_dtm, 100)
head(imdb_freq_words)
## [1] "abandon" "abil"    "abl"     "about"   "absolut" "absurd"

length(imdb_freq_words)
## [1] 2018
```

```R
# train, test 분할
set.seed(1)
train_ind = sample(c(1:12000), 8400)

imdb_dtm_train = imdb_dtm[train_ind, imdb_freq_words]
imdb_train_labels = imdb_sub[train_ind, 2]

imdb_dtm_test = imdb_dtm[-train_ind, imdb_freq_words]
imdb_test_labels = imdb_sub[-train_ind, 2]

table(imdb_train_labels); table(imdb_test_labels)
## imdb_train_labels
## negative positive 
##     4270     4130
## imdb_test_labels
## negative positive 
##     1768     1832
```

```R
# naiveBayes 적용하기
library(e1071)

convert_counts = function(x){
  x = ifelse(x>0, "Yes", "NO")
}

imdb_train = apply(imdb_dtm_train, MARGIN = 2, convert_counts)
imdb_test = apply(imdb_dtm_test, MARGIN = 2, convert_counts)

imdb_classifier = naiveBayes(imdb_train, imdb_train_labels)
```

```R
# 생성된 모델을 바탕으로 test data결과 예측하기
imdb_test_pred = predict(imdb_classifier, imdb_test)
head(imdb_test_pred)
## [1] positive positive negative negative positive negative
## Levels: negative positive

table(imdb_test_pred, imdb_test_labels)
##               imdb_test_labels
## imdb_test_pred negative positive
##       negative     1493      281
##       positive      275     1551
```



## 2.
IMDB 데이터셋 사용

- 리뷰 데이터 전처리(불용어, 특수문자, 숫자 등 제거, 어근 동일화 등)
- DTM 생성
- train/test data set 분리
- 열의 크기가 매우 크므로 최소 10번 이상 등장한 단어 대상으로 할 것
- kmeans 기반 클러스터링(2개 그룹), 감성분석사전(bing or nrc)기반 클러스터링
- test data의 정확도(긍/부정) 출력

```R
# kmeans 까지 수행

imdb_kmeans = kmeans(imdb_dtm_train, centers = 2)
summary(imdb_kmeans)
##              Length Class  Mode   
## cluster      8400   -none- numeric
## centers      4036   -none- numeric
## totss           1   -none- numeric
## withinss        2   -none- numeric
## tot.withinss    1   -none- numeric
## betweenss       1   -none- numeric
## size            2   -none- numeric
## iter            1   -none- numeric
## ifault          1   -none- numeric
```

```R
table(imdb_kmeans$cluster, imdb_train_labels)
##    imdb_train_labels
##     negative positive
##   1     3509     3441
##   2      761      689
```

