[toc]

# Day58 Scikitlearn을 활용한 머신러닝(10)

- tensorflow 설치
  - anaconda prompt : `pip install tensorflow==1.15.0`
    - 최신 버전 설치시 : `pip uninstall tensorflow`
    - 삭제할 경우 : `pip uninstall tensorflow`
- gensim 설치 : `pip install gensim`
  - 자연어 처리에 사용하는 패키지
- nltk : 영어에 대한 자연어 처리 패키지 `pip install nltk`
  - jupyter notebook

```python
import nltk
nltk.__version__
# > '3.4.5'

nltk.download() # nltk data 다운로드
# > showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml
# > True
```

> - nltk.download('treebank') # treebank 별도 설치
> - github.com/nltk/nltk/nltk_data에서 다운해서 경로에 저장 가능

- konlpy : `pip install konlpy`
  - jupyter notebook에서 `konlpy.__version__`으로 설치 확인
  - 설치시 에러가 발생하면 보통 Java가 설치되지 않은 것
    - 자바 설치 후
    - 제어판 > 시스템 및 보안 > 시스템 > 고급 시스템 설정 > 고급 > 환경 변수
      새로 만들기(N) > JAVA_HOME 환경 변수 추가 및 자바 설치 경로 지정
- JPype : 자바와 파이썬 연결
  - prompt에서 `python --version`으로 version 확인 후
  - https://www.lfd.uci.edu/~gohlke/pythonlibs/#jpype
  - 에서 python version에 맞는 파일 다운로드
    - 예. python이 3.7.x 일 경우, JPype1- ~~ -cp37m-win_amd64.whl 다운
    - 여기서 64는 컴퓨터 비트를 의미
  - prompt의 경로를 JPype 파일이 있는 경로로 변경 후
  - `pip install JPype1- ~~ -cp37m-win_amd64.whl` 실행
- `pip list` : 설치되어있는 패키지 목록 확인



## 토큰화, 정제, 정규화

- 토큰화 : 어절, 단어, 문장, 문단 (일반적으로 단어를 의미)
- 단어토큰화 : 구두점, 특수문자 제거

```python
from nltk.tokenize import word_tokenize
print(word_tokenize("Mr. Jone's Don't be ")) # 가장 많이 사용
# > ['Mr.', 'Jone', "'s", 'Do', "n't", 'be']

from nltk.tokenize import WordPunctTokenizer
print(WordPunctTokenizer().tokenize("Mr. Jone's Don't be "))
# > ['Mr', '.', 'Jone', "'", 's', 'Don', "'", 't', 'be']

from tensorflow.keras.preprocessing.text import text_to_word_sequence
print(text_to_word_sequence("Mr. Jone's Don't be "))
# > ['mr', "jone's", "don't", 'be']

from nltk.tokenize import TreebankWordTokenizer
twt = TreebankWordTokenizer()
twt.tokenize("Mr. Jone's Don't be ")
# > ['Mr.', 'Jone', "'s", 'Do', "n't", 'be']
```

> 중요한 건 정규표현식

```python
text = "Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects."

from nltk.tokenize import sent_tokenize
sent_tokenize(text)
# > ['Python is an interpreted, high-level, general-purpose programming language.',
# >  "Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.",
# >  'Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.']
```

- 조사 : 그녀가(에게, 를, 와, 는, ...)
- 아버지가방에들어가신다.
- 토큰화 작업 : 형태소(가장 작은 말의 단위)고려
    - 자립형태소/의존형태소
    - ex. 길동이가 파이썬을 합니다.
        - 자립형태소 : 길동이, 파이썬
        - 의존형태소(조사, 어미) : 가, 을, 합니다.
- 품사 태깅(tagging)
  - 토큰화 과정에서 단어가 어떤 품사로 쓰였는지 구분하는 작업

```python
text = "Python is an interpreted, high-level, general-purpose programming language."
print(word_tokenize(text))
# > ['Python', 'is', 'an', 'interpreted', ',', 'high-level', ',', 'general-purpose', 'programming', 'language', '.']

from nltk.tag import pos_tag
pos_tag(word_tokenize(text))
# > [('Python', 'NNP'),
# >  ('is', 'VBZ'),
# >  ('an', 'DT'),
# >  ('interpreted', 'JJ'),
# >  (',', ','),
# >  ('high-level', 'JJ'),
# >  (',', ','),
# >  ('general-purpose', 'JJ'),
# >  ('programming', 'NN'),
# >  ('language', 'NN'),
# >  ('.', '.')]
```

### 형태소 분석

- 형태소 분석기 : Okt, Mecab, Kkma, ...
- 단어토큰화가 아님, 형태소 토큰화를 하는 것임
- 영어는 단어 토큰화이지만, 한국어는 형태소 토큰화

```python
from konlpy.tag import Okt
```

> Okt와 JPype버전이 맞지 않는 경우 에러가 발생할 수 있음
>
> jpype1 버전을 0.7.0으로 재설치
>
> `pip install jpype1==0.7.0`

- morphs : 형태소 추출
- pos : 품사
- nouns : 명사

```python
okt = Okt()
print(okt.morphs("오늘은 수요일, 내일은 목요일입니다."))
# > ['오늘', '은', '수요일', ',', '내일', '은', '목요일', '입니다', '.']

print(okt.pos("오늘은 수요일, 내일은 목요일입니다."))
# > [('오늘', 'Noun'), ('은', 'Josa'), ('수요일', 'Noun'), (',', 'Punctuation'), ('내일', 'Noun'), ('은', 'Josa'), ('목요일', 'Noun'), ('입니다', 'Adjective'), ('.', 'Punctuation')]

print(okt.nouns("오늘은 수요일, 내일은 목요일입니다."))
# > ['오늘', '수요일', '내일', '목요일']
```

```python
from konlpy.tag import Kkma
kma = Kkma()
kma.morphs("오늘은 수요일, 내일은 목요일입니다.")
# > ['오늘', '은', '수요일', ',', '내일', '은', '목요일', '입', '니', '다', '.']
```

- 데이터 정제(전처리)
     - 불필요한 형태소 제거
  - 단어 통일(밥, 식사, ...)
  - 정규표현식을 이용 (\w{1,2})

## LSA(Latent Semantic Analysis)

- 잠재적인 의미 추출
- ex.

| 메뉴                  | 토픽   | DTM  | 떡만두 | 김밥 | 피자 | 햄버거 | 쿠키 |
| --------------------- | ------ | ---- | ------ | ---- | ---- | ------ | ---- |
| #1 떡만두             | 한국식 |      | 1      | 0    | 0    | 0      | 0    |
| #2 김밥               | 한국식 |      | 0      | 1    | 0    | 0      | 0    |
| #3 떡만두, 김밥       | 한국식 |      | 1      | 1    | 0    | 0      | 0    |
| #4 피자               | 서양식 |      | 0      | 0    | 1    | 0      | 0    |
| #5 피자, 햄버거, 쿠키 | 서양식 |      | 0      | 0    | 1    | 1      | 1    |
| #6 햄버거             | 서양식 |      | 0      | 0    | 0    | 1      | 0    |

- 유사도(\#4 피자, \#6 햄버거) = 0(내적결과)
- 유사도(피자, 김밥) = 0
- => 한계발생
- **이를 극복하기 위해서 LSA 등장**


- LSA : 토픽으로 유사도를 구하는 알고리즘
- SVD(Singular Value Decomposition) : 특이값 분해
    - 주어진 행렬을 여러개의 곱으로 나누는 작업
    - A 행렬 : m\*n 행렬, 행렬 3개의 곱 형태로 분해
        - A = U \* Sigma * V^T
        - U : m \* m 직교행렬
            - 직교행렬 : U \* U^T or U^T \* U 이 단위행렬인 경우
            - 토픽에 대한 단어 행렬
        - Simga : m \* n 직사각대각행렬
            - cf. 대각원소들이 중요함
            - 토픽 강도 (강한 순으로 되어 있음)
        - V : n \* n 직교행렬
            - 토픽에 대한 문서 행렬


- cf. 행렬
    - 전치행렬 : A^T
    - 단위행렬 : I

- 토픽의 개수 k를 결정하여 Sigma 행렬을 k \* k행렬(1:k)로  절단하여 사용한다. 

### 실습

```python
import numpy as np
A = np.array([[0,0,0,1,0,1,1,0,0],
             [0,0,0,1,1,0,1,0,0],
             [0,1,1,0,2,0,0,0,0],
             [1,0,0,0,0,0,0,1,1]]) # DTM
A.shape
# > (4, 9)
```

#### full SVD

```python
U, s, VT = np.linalg.svd(A, full_matrices=True)
```
```python
U.shape
# > (4, 4)

U
# > array([[-2.39751712e-01,  7.51083898e-01,  5.48519247e-17,
# >         -6.15135834e-01],
# >        [-5.06077194e-01,  4.44029376e-01, -8.22778870e-17,
# >          7.39407727e-01],
# >        [-8.28495619e-01, -4.88580485e-01, -7.47642703e-17,
# >         -2.73649629e-01],
# >        [-9.04299898e-17, -4.11929620e-17,  1.00000000e+00,
# >          7.41190750e-17]])

U.round(2) # 직교행렬
# > array([[-0.24,  0.75,  0.  , -0.62],
# >        [-0.51,  0.44, -0.  ,  0.74],
# >        [-0.83, -0.49, -0.  , -0.27],
# >        [-0.  , -0.  ,  1.  ,  0.  ]])
```

```python
s.round(2)
# > array([2.69, 2.05, 1.73, 0.77])

s.shape
# > (4,)

S = np.zeros([4,9])
S
# > array([[0., 0., 0., 0., 0., 0., 0., 0., 0.],
# >        [0., 0., 0., 0., 0., 0., 0., 0., 0.],
# >        [0., 0., 0., 0., 0., 0., 0., 0., 0.],
# >        [0., 0., 0., 0., 0., 0., 0., 0., 0.]])

np.diag(s)
# > array([[2.68731789, 0.        , 0.        , 0.        ],
# >        [0.        , 2.04508425, 0.        , 0.        ],
# >        [0.        , 0.        , 1.73205081, 0.        ],
# >        [0.        , 0.        , 0.        , 0.77197992]])

S[:4, :4] = np.diag(s.round(2))
S # 특이값 행렬
# > array([[2.69, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],
# >        [0.  , 2.05, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],
# >        [0.  , 0.  , 1.73, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],
# >        [0.  , 0.  , 0.  , 0.77, 0.  , 0.  , 0.  , 0.  , 0.  ]])
```

```python
VT.round(2)
# > array([[-0.  , -0.31, -0.31, -0.28, -0.8 , -0.09, -0.28, -0.  , -0.  ],
# >        [ 0.  , -0.24, -0.24,  0.58, -0.26,  0.37,  0.58, -0.  , -0.  ],
# >        [ 0.58, -0.  ,  0.  ,  0.  , -0.  ,  0.  , -0.  ,  0.58,  0.58],
# >        [ 0.  , -0.35, -0.35,  0.16,  0.25, -0.8 ,  0.16, -0.  , -0.  ],
# >        [-0.  , -0.78, -0.01, -0.2 ,  0.4 ,  0.4 , -0.2 ,  0.  ,  0.  ],
# >        [-0.29,  0.31, -0.78, -0.24,  0.23,  0.23,  0.01,  0.14,  0.14],
# >        [-0.29, -0.1 ,  0.26, -0.59, -0.08, -0.08,  0.66,  0.14,  0.14],
# >        [-0.5 , -0.06,  0.15,  0.24, -0.05, -0.05, -0.19,  0.75, -0.25],
# >        [-0.5 , -0.06,  0.15,  0.24, -0.05, -0.05, -0.19, -0.25,  0.75]])
```

```python
np.dot(np.dot(U,S),VT)
# > array([[ 5.52842847e-17, -1.11554830e-03, -1.11554830e-03,
# >          1.00253215e+00, -1.41824901e-04,  1.00044288e+00,
# >          1.00253215e+00,  5.46270875e-17,  5.46270875e-17],
# >        [ 2.53166630e-16,  4.15949249e-04,  4.15949249e-04,
# >          1.00141660e+00,  1.00015923e+00,  2.08927170e-03,
# >          1.00141660e+00, -8.21208495e-17, -8.21208495e-17],
# >        [ 1.08203234e-16,  1.00106681e+00,  1.00106681e+00,
# >         -6.99599051e-04,  2.00254956e+00, -1.11554830e-03,
# >         -6.99599051e-04,  6.88063336e-17,  6.88063336e-17],
# >        [ 9.98815966e-01, -3.34702175e-17,  2.08981987e-16,
# >          3.72933769e-16, -2.38142277e-17,  2.43669552e-16,
# >         -1.12033979e-17,  9.98815966e-01,  9.98815966e-01]])

np.dot(np.dot(U,S),VT).shape
# > (4, 9)

np.dot(np.dot(U,S),VT).round(2)
# > array([[ 0., -0., -0.,  1., -0.,  1.,  1.,  0.,  0.],
# >        [ 0.,  0.,  0.,  1.,  1.,  0.,  1., -0., -0.],
# >        [ 0.,  1.,  1., -0.,  2., -0., -0.,  0.,  0.],
# >        [ 1., -0.,  0.,  0., -0.,  0., -0.,  1.,  1.]])
```

- np.allclose() : 두 변수가 같은지 비교해줌

```python
A
# > array([[0, 0, 0, 1, 0, 1, 1, 0, 0],
# >        [0, 0, 0, 1, 1, 0, 1, 0, 0],
# >        [0, 1, 1, 0, 2, 0, 0, 0, 0],
# >        [1, 0, 0, 0, 0, 0, 0, 1, 1]])

np.allclose(A, np.dot(np.dot(U,S), VT).round(2))
# > True
```

#### truncated(절단된) SVD

- 토픽 2개

```python
S = S[:2, :2]
S
# > array([[2.69, 0.  ],
# >        [0.  , 2.05]])
```

```python
U.shape # 4,4
U[:, :2].shape # 4,2 문서*토픽
# U의 의미는 4개 문서 각각에 대한 잠재된 의미를 표현하고 있는 수치 문서 벡터
U = U[:, :2]
U.round(2)
# > array([[-0.24,  0.75],
# >        [-0.51,  0.44],
# >        [-0.83, -0.49],
# >        [-0.  , -0.  ]])
```

```python
VT.shape # 9,9
VT[:2, :].shape # 2,9 토픽*단어개수
# VT의 각 열은 잠재의미를 나타내기 위한 수치화된 단어 벡터
VT = VT[:2, :]
VT.round(2)
# > array([[-0.  , -0.31, -0.31, -0.28, -0.8 , -0.09, -0.28, -0.  , -0.  ],
# >        [ 0.  , -0.24, -0.24,  0.58, -0.26,  0.37,  0.58, -0.  , -0.  ]])
```

```python
Aprime = np.dot(np.dot(U,S), VT)
Aprime.round(2)
# > array([[ 0.  , -0.17, -0.17,  1.08,  0.12,  0.62,  1.08, -0.  , -0.  ],
# >        [ 0.  ,  0.2 ,  0.2 ,  0.91,  0.86,  0.46,  0.91,  0.  ,  0.  ],
# >        [ 0.  ,  0.93,  0.93,  0.03,  2.05, -0.17,  0.03,  0.  ,  0.  ],
# >        [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  , -0.  ,  0.  ,  0.  ,  0.  ]])

A
# > array([[0, 0, 0, 1, 0, 1, 1, 0, 0],
# >        [0, 0, 0, 1, 1, 0, 1, 0, 0],
# >        [0, 1, 1, 0, 2, 0, 0, 0, 0],
# >        [1, 0, 0, 0, 0, 0, 0, 1, 1]])
```

```python
np.dot(S, VT).round(2) # 각 행을 각 토픽으로 보면 된다.
# > array([[-0.  , -0.83, -0.83, -0.75, -2.17, -0.24, -0.75, -0.  , -0.  ],
# >        [ 0.  , -0.49, -0.49,  1.2 , -0.53,  0.75,  1.2 , -0.  , -0.  ]])
```

## cosine 유사도

- DTM

|         | 돈   | 사랑 | 저는 | 좋아요 |
| ------- | ---- | ---- | ---- | ------ |
| 줄거리1 | 0    | 1    | 1    | 1      |
| 줄거리2 | 1    | 0    | 1    | 1      |
| 줄거리3 | 2    | 0    | 2    | 2      |

```python
from numpy.linalg import norm

mov1 = np.array([0,1,1,1])
mov2 = np.array([1,0,1,1])
mov3 = np.array([2,0,2,2])
```

```python
np.dot(mov1, mov2) # 두 벡터의 내적
# > 2

# norm() : 벡터의 크기를 구하는 함수
norm(mov1)
# > 1.7320508075688772

norm(mov1)*norm(mov2)
# > 2.9999999999999996

np.dot(mov1, mov2) / (norm(mov1)*norm(mov2)) # cosine 유사도
# > 0.6666666666666667
```

```python
def cos_sim(X, Y) :
    return np.dot(X, Y) / (norm(X)*norm(Y))

print(cos_sim(mov1, mov2))
print(cos_sim(mov2, mov3))
print(cos_sim(mov1, mov3))
# > 0.6666666666666667
# > 1.0000000000000002
# > 0.6666666666666667
```

## kaggle의 The Movies Dataset 중 movies_metadata.csv

```python
import pandas as pd

path = "../data_for_analysis/"
data = pd.read_csv(path + "the-movies-dataset/movies_metadata.csv")
data.shape
# > (45466, 24)

# 데이터가 너무 많으므로 20,000개 사용
data = data.head(20000)
data['overview']
# > 0        Led by Woody, Andy's toys live happily in his ...
# > 1        When siblings Judy and Peter discover an encha...
# > 2        A family wedding reignites the ancient feud be...
# > 3        Cheated on, mistreated and stepped on, the wom...
# > 4        Just when George Banks has recovered from his ...
# >                                ...                        
# > 19995    Dissidents in a French colony attack a police ...
# > 19996    A young mother Nina and her son Enzo find them...
# > 19997    An in-depth analysis of the relationship betwe...
# > 19998    Follows the life and work of animator Lotte Re...
# > 19999    An in-depth look at the genesis, production, a...
# > Name: overview, Length: 20000, dtype: object

data['overview'].isnull().sum()
# > 135

data['overview'] = data['overview'].fillna('')
data['overview'].isnull().sum()
# > 0
```

### TFIDF 변환

```python
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(stop_words='english') # english에 해당되는 stop word제거
tfidf_mat = tfidf.fit_transform(data['overview'])
tfidf_mat.shape
# > (20000, 47487)
```

```python
data.info()
# > <class 'pandas.core.frame.DataFrame'>
# > RangeIndex: 20000 entries, 0 to 19999
# > Data columns (total 24 columns):
# > adult                    20000 non-null object
# > belongs_to_collection    2399 non-null object
# > budget                   20000 non-null object
# > genres                   20000 non-null object
# > homepage                 3055 non-null object
# > id                       20000 non-null object
# > imdb_id                  19993 non-null object
# > original_language        19999 non-null object
# > original_title           20000 non-null object
# > overview                 20000 non-null object
# > popularity               19998 non-null object
# > poster_path              19907 non-null object
# > production_companies     19999 non-null object
# > production_countries     19999 non-null object
# > release_date             19983 non-null object
# > revenue                  19998 non-null float64
# > runtime                  19971 non-null float64
# > spoken_languages         19998 non-null object
# > status                   19979 non-null object
# > tagline                  11706 non-null object
# > title                    19998 non-null object
# > video                    19998 non-null object
# > vote_average             19998 non-null float64
# > vote_count               19998 non-null float64
# > dtypes: float64(4), object(20)
# > memory usage: 3.7+ MB

data['title']
# > 0                                                Toy Story
# > 1                                                  Jumanji
# > 2                                         Grumpier Old Men
# > 3                                        Waiting to Exhale
# > 4                              Father of the Bride Part II
# >                                ...                        
# > 19995                                            Rebellion
# > 19996                                           Versailles
# > 19997                                      Two in the Wave
# > 19998    Lotte Reiniger: Homage to the Inventor of the ...
# > 19999    RKO Production 601: The Making of 'Kong, the E...
# > Name: title, Length: 20000, dtype: object
```

# 연습문제

- Toy Story와 가장 코사인 유사도가 큰 영화 10편을 추출
- => 영화의 제목과 유사도 출력

```python
tfidf_array = tfidf_mat.toarray()
tfidf_array
# > array([[0., 0., 0., ..., 0., 0., 0.],
# >        [0., 0., 0., ..., 0., 0., 0.],
# >        [0., 0., 0., ..., 0., 0., 0.],
# >        ...,
# >        [0., 0., 0., ..., 0., 0., 0.],
# >        [0., 0., 0., ..., 0., 0., 0.],
# >        [0., 0., 0., ..., 0., 0., 0.]])

np.dot(tfidf_array[0], tfidf_array[1])
# > 0.01575747731678539

norm(tfidf_array[0])
# > 1.0
```

```python
def cos_top10(tfidf_data, movie) :
    resList = list()
    
    for i in range(len(tfidf_data)) :
        if i == movie : continue
        cos = np.dot(tfidf_data[movie], tfidf_data[i]) / (norm(tfidf_data[movie])*norm(tfidf_data[i]))
        
        resList.append((cos, i))
    
    resList.sort()
    resList.reverse()
    return resList[:10]
```

```python
result = cos_top10(tfidf_array, 0)
result
# > [(0.11798196632714826, 19976),
# >  (0.08454103120942906, 16585),
# >  (0.07448744944250994, 19875),
# >  (0.06369332191395058, 18060),
# >  (0.06099673588133421, 19992),
# >  (0.0524938793119962, 17645),
# >  (0.027105644611392515, 19969),
# >  (0.024584793469282384, 19996),
# >  (0.019853155734699927, 19968),
# >  (0.01969395366887644, 17917)]
```

```python
movieList = list()
for sc, i in result :
    movieList.append((data['title'][i], sc))
movieList
# > [('Dragon Fight', 0.11798196632714826),
# >  ('I Spit on Your Grave', 0.08454103120942906),
# >  ('Raw Deal: A Question Of Consent', 0.07448744944250994),
# >  ('Bone', 0.06369332191395058),
# >  ('How to Make Love to a Woman', 0.06099673588133421),
# >  ('Passing Fancy', 0.0524938793119962),
# >  ('Miss Annie Rooney', 0.027105644611392515),
# >  ('Versailles', 0.024584793469282384),
# >  ('Little Miss Broadway', 0.019853155734699927),
# >  ('Small Town Girl', 0.01969395366887644)]
```

